{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class State(BaseModel):\n",
    "    question : str = Field(description= \"Question given be the user\")\n",
    "    answer : str = Field(description=\"Answer given by the Application\")\n",
    "    chat_summary : str = Field(description=\"Chat history maintained by the applciation\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filetype\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def file_router(file):\n",
    "    kind = filetype.guess(file)\n",
    "    if kind is None:\n",
    "        return \"Unknown\"\n",
    "    file_type =  kind.mime\n",
    "    if file_type.startswith(\"image/\"):\n",
    "        return 'imagesingle'\n",
    "\n",
    "    # or else this is pdf and if there is images kind of pdf then return 'image' or text based then return pdf\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    if not len(docs[0].page_content):\n",
    "        return 'imagepdf'\n",
    "    \n",
    "    else :\n",
    "        return 'pdf'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "from io import BytesIO\n",
    "\n",
    "model = llm = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash')\n",
    "\n",
    "def encode_image(image) -> str:\n",
    "    \"\"\"Encode a PIL image to base64 string.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize(model, base64_image: str, prompt: str) -> str:\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    msg = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "image_prompt = \"\"\"You are a highly meticulous AI assistant that extracts and summarizes every possible piece of visual information from an image without omitting any detail.  \n",
    "    Your task is to generate an exhaustive, structured summary of the image that captures all the text, visual elements, layout, colors (if relevant), numbers, figures, and any context or formatting that might be useful.  \n",
    "    Do not generalize or paraphrase â€” capture the content exactly as it appears. Use bullet points, lists, or structured sections (e.g., titles, tables, headers, footnotes) to organize your summary.  \n",
    "\n",
    "    Be especially attentive to:\n",
    "    - All visible text, including headers, footnotes, and marginal notes  \n",
    "    - Tables: Capture each row and column verbatim including headers and cell values  \n",
    "    - Graphs/Charts: Explain all axes, labels, legends, data points, patterns, and conclusions  \n",
    "    - Visual layout and structure: Describe how content is arranged (e.g., two-column layout, centered title, left-aligned figure)  \n",
    "    - Icons, logos, or images embedded within the image: Describe them accurately  \n",
    "    - Fonts, colors, and emphasis (e.g., bold, italic, underlined) if they seem meaningful  \n",
    "    - Dates, numbers, symbols, or special formatting exactly as shown  \n",
    "    - If the image is a document or scanned page, preserve hierarchy and document structure  \n",
    "\n",
    "    Output the result in structured markdown with clear section headers (e.g., \"Header\", \"Table 1\", \"Figure Description\", \"Text Body\", \"Footnotes\").  \n",
    "    Your goal is to allow someone to fully understand the image without seeing it, preserving maximum detail for use in downstream AI models or search systems.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def image_handler(image):\n",
    "\n",
    "    base64_img = encode_image(image)\n",
    "    summary = image_summarize(model, base64_img, prompt=image_prompt)\n",
    "    with open('example.txt','w') as f:\n",
    "        f.write(summary)\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_handler_append(image):\n",
    "    base64_img = encode_image(image)\n",
    "    summary = image_summarize(model, base64_img, prompt=image_prompt)\n",
    "    \n",
    "    # Append instead of overwrite\n",
    "    with open('example.txt', 'a') as f:\n",
    "        f.write(summary + '\\n')  # Add newline for separation\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def vectorize_text(text:str):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = 600,chunk_overlap = 50)\n",
    "\n",
    "    docs = splitter.split_text(text)\n",
    "\n",
    "    vectorstore = Chroma.from_texts(docs,embedding= HuggingFaceEmbeddings())\n",
    "\n",
    "    return vectorstore\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_single_image(image):\n",
    "    summary = image_handler(image)\n",
    "    return vectorize_text(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "def vectorize_multiple_images(image):\n",
    "\n",
    "    images = convert_from_path(image)\n",
    "    summary = ''\n",
    "    for i, image in enumerate(images):\n",
    "        filename = f\"page_{i + 1}.png\"\n",
    "        print(filename)\n",
    "        image.save(filename, \"PNG\")\n",
    "        if filename == 'page_1.png':\n",
    "            summary = image_handler(image)\n",
    "        else:\n",
    "            summary += image_handler_append(image)\n",
    "    \n",
    "    return vectorize_text(summary)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def vectorize_docs(filepath):\n",
    "    loader = PyPDFLoader(filepath)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size= 600,chunk_overlap= 80)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vectorstore = Chroma.from_documents(chunks,HuggingFaceEmbeddings())\n",
    "    return vectorstore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(filepath):\n",
    "    type_of_file = file_router(filepath)\n",
    "    print(type_of_file)\n",
    "    if type_of_file == 'imagesingle':\n",
    "        return vectorize_single_image(filepath)\n",
    "    elif type_of_file == 'imagepdf':\n",
    "        return vectorize_multiple_images(filepath)\n",
    "    else :\n",
    "        return vectorize_docs(filepath)\n",
    "        \n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6720/2576594015.py:8: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "\n",
    "retriever = vectorize('shreyankresume.pdf').as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever |format_docs , 'question':RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question} \"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the candidate, Shreyank, has worked on the following projects:\n",
      "\n",
      "*   **Harini: Intelligent AI Scheduling Agent:** A fully autonomous scheduling agent using LangChain and LangGraph, automating calendar updates, confirmations, and follow-ups.\n",
      "*   **AI interviewer:** With TTS abd STT and LLM\n",
      "*   **ATSResumeAnalyze:** With LLM Integration\n",
      "*   **MusicGenerator:** Using LSTM networks\n",
      "*   **TelegramBot:** With Mistral AI\n",
      "*   **CallFraudDetector:** Using Naive Bayes network\n",
      "*   **CourseRecommender:** Using Vector database and Langchain\n",
      "*   **VirtualLawyer:** RAG based application with LLM\n",
      "* Built 20+ AI-powered chatbots using LangChain and LangGraph, demonstrating advanced multi-agent system skills.\n"
     ]
    }
   ],
   "source": [
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "\n",
    "\n",
    "fallback_prompt = \"\"\"\n",
    "You are a helpful and honest assistant working within a RAG (Retrieval-Augmented Generation) system. You attempted to answer a user's question based on retrieved knowledge, but the information may be incomplete, irrelevant, or not confidently grounded.\n",
    "\n",
    "Your goal now is to:\n",
    "- Review the chat summary to understand ongoing conversation context.\n",
    "- Review the retrieved context.\n",
    "- Evaluate the initially attempted answer.\n",
    "- If the context was insufficient or the answer is vague, provide a polite, thoughtful fallback response.\n",
    "- If helpful, ask the user to clarify or reformulate their question.\n",
    "\n",
    "---\n",
    "\n",
    "Chat Summary (Conversation So Far):\n",
    "{chat_summary}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "Initial Answer Attempted:\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "Just return 'fall_back' if the answer is NOT good enough.\n",
    "Return 'continue' if the answer is relevant and sufficient.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "fall_back_prompt = ChatPromptTemplate.from_template(fallback_prompt)\n",
    "\n",
    "def fallback_node(state: State) -> dict:\n",
    "    chat_summary = state.chat_summary\n",
    "    question = state.question\n",
    "    answer = state.answer\n",
    "\n",
    "    # Combine into a runnable chain\n",
    "    fallback_chain = fall_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Provide input variables to the chain\n",
    "    decision = fallback_chain.invoke({\n",
    "        \"chat_summary\": chat_summary,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "    decision = decision.strip().lower()\n",
    "\n",
    "    if decision == \"continue\":\n",
    "        return {\"answer\": answer}\n",
    "    else:\n",
    "        fallback_response = (\n",
    "            \"Apologies! It seems I don't have enough reliable information to confidently answer your question right now. \"\n",
    "            \"This might be due to insufficient or unclear context. \"\n",
    "            \"Please consider rephrasing your question or using a more advanced model for better results.\"\n",
    "        )\n",
    "        return {\"answer\": fallback_response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_prompt = \"\"\"\n",
    "You are an expert summarizer. Summarize the entire conversation, including the latest question and answer pair, while preserving the key points.\n",
    "\n",
    "Previous Summary ignore if its not there:\n",
    "{previous_summary}\n",
    "\n",
    "New Question and Answer:\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "---\n",
    "\n",
    "Updated Summary:\n",
    "\"\"\"\n",
    "\n",
    "import dotenv \n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-flash')\n",
    "\n",
    "def summarizer_node(state:State):\n",
    "    \n",
    "    question = state.question\n",
    "    answer = state.answer\n",
    "    previous_summary = state.chat_summary \n",
    "    \n",
    "    \n",
    "    prompt = summarizer_prompt.format(\n",
    "        previous_summary=previous_summary,\n",
    "        question=question,\n",
    "        answer=answer\n",
    "    )\n",
    "\n",
    "    \n",
    "    updated_summary = llm.invoke(prompt)\n",
    "\n",
    "    \n",
    "    return {\"updated_summary\": updated_summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Literal \n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "class HallucinationGrader(BaseModel):\n",
    "    \"Binary score for hallucination check in llm's response\"\n",
    "\n",
    "    grade: Literal[\"yes\", \"no\"] = Field(\n",
    "        ..., description=\"'yes' if the llm's reponse is hallucinated otherwise 'no'\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader_system_prompt_template = (\n",
    "    '''You are a grader assessing whether a response from an LLM is grounded in the given question and the context provided during the retrieval process.\n",
    "    You will be given the following inputs:\n",
    "    - The question asked by the user is {question}\n",
    "\n",
    "    - The answer provided by the RAG application is on the basis of the context is \n",
    "    {answer}\n",
    "\n",
    "    \n",
    "    Your task is to determine if the LLM's response is based on the context (implicitly retrieved).\n",
    "    If the LLM's response does not align with the question or context (introduces unrelated information), \n",
    "    it is considered a hallucination. In such cases, give a score of 'yes' (hallucinated).\n",
    "    If the LLM's response is grounded in the context and consistent with the question, give a score of 'no' (not hallucinated).\n",
    "    \n",
    "    Just provide your answer in the following JSON format:\n",
    "     grade: yes  or  grade: no \n",
    "    No additional explanation is needed.'''\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "hallucination_grader_prompt = ChatPromptTemplate.from_template(hallucination_grader_system_prompt_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_grader(state: State):\n",
    "    hallucination_grader_chain = (\n",
    "        hallucination_grader_prompt\n",
    "        | llm.with_structured_output(HallucinationGrader, method=\"json_mode\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    graded_response = hallucination_grader_chain.invoke({\n",
    "        'question': state.question,\n",
    "        'answer': state.answer\n",
    "    })\n",
    "    \n",
    "    return graded_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AnswerGrader(BaseModel):\n",
    "    '''Binary score for an answer check based on a query.'''\n",
    "\n",
    "    grade: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"'yes' if the provided answer is an actual answer to the query otherwise 'no',\"\n",
    "    )\n",
    "\n",
    "\n",
    "answer_grader_system_prompt_template = (\n",
    "    '''\n",
    "    You are a grader assessing whether the provided answer is a valid and relevant response to the given query.\n",
    "    If the provided answer addresses the query correctly, give a score of 'yes'.\n",
    "    If the provided answer does not answer the query or is irrelevant, give a score of 'no'.\n",
    "    Just provide your answer in the following JSON format:\n",
    "    grade: yes or grade: no\n",
    "    No additional explanation is needed.'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_grader_prompt = ChatPromptTemplate.from_template(answer_grader_system_prompt_template)\n",
    "answer_grader_chain = answer_grader_prompt | llm.with_structured_output(\n",
    "        AnswerGrader, method=\"json_mode\"\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_grader(state:State):\n",
    "    answer = state.answer\n",
    "    question = state.question\n",
    "    grade = answer_grader_chain.invoke({'question':question,'answer':answer})\n",
    "    return grade\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hallucination_and_answer_relevance_check(state:State):\n",
    "    \n",
    "\n",
    "    hallucination_grade = hallucination_grader(state)\n",
    "    print(f\"hallucinatio grade is {hallucination_grade}\")\n",
    "    if hallucination_grade.grade == \"no\":\n",
    "        print(\"---Hallucination check passed---\")\n",
    "        answer_relevance_grade = answer_grader(state)\n",
    "        print(answer_relevance_grade)\n",
    "        if answer_relevance_grade.grade == \"yes\":\n",
    "            print(\"---Answer is relevant to question---\\n\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---Answer is not relevant to question---\")\n",
    "            return \"not_useful\"\n",
    "    print(\"---Hallucination check failed---\")\n",
    "    return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(state:State):\n",
    "    query = state.question\n",
    "    answer = final_rag_chain.invoke({'question':query})\n",
    "    return {'answer':answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Add your nodes\n",
    "builder.add_node(\"rag_answer\", rag_answer)\n",
    "builder.add_node(\"summarizer_node\", summarizer_node)\n",
    "builder.add_node(\"fallback_node\", fallback_node)\n",
    "builder.add_node(hallucination_grader)\n",
    "builder.add_node(answer_grader)\n",
    "\n",
    "\n",
    "# Define your edges\n",
    "builder.add_edge(START, \"rag_answer\")\n",
    "builder.add_edge(\"rag_answer\", \"summarizer_node\")\n",
    "builder.add_edge('summarizer_node','hallucination_grader')\n",
    "builder.add_edge('hallucination_grader','answer_grader')\n",
    "\n",
    "# Add conditional edge using the function as a router\n",
    "builder.add_conditional_edges(\n",
    "    \"answer_grader\",\n",
    "    hallucination_and_answer_relevance_check,  \n",
    "   {\n",
    "        \"useful\": END,  \n",
    "        \"generate\": \"rag_answer\", \n",
    "        \"not_useful\": \"fallback_node\"\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"fallback_node\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = State(question = \"What job role is perfect for the candidiate\",answer = '',chat_summary = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hallucinatio grade is grade='no'\n",
      "---Hallucination check passed---\n",
      "grade='yes'\n",
      "---Answer is relevant to question---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What job role is perfect for the candidiate',\n",
       " 'answer': 'Based on the provided documents, Shreyank would be a good fit for roles such as:\\n\\n*   **AI Engineer/Developer:** His proficiency in Deep Learning frameworks (PyTorch, TensorFlow), Generative & Agentic AI tools (LangChain, LangGraph, OpenAI, Groq, Anthropic), and experience building AI-powered chatbots and applications strongly suggest suitability for this role.\\n*   **Backend Developer (AI Focus):** His experience with Java, Spring Boot, Spring MVC, Spring REST, FastAPI, and Docker, coupled with his AI skills, makes him a strong candidate for backend development roles that involve integrating AI functionalities.\\n*   **Machine Learning Engineer:** His achievements in AI hackathons, experience with natural language processing, computer vision, and building AI projects like the \"AI interviewer\" and \"CallFraudDetector\" highlight his capabilities in machine learning.\\n*   **Full Stack Developer (AI Specialization):** Given his backend skills and experience with AI projects, he could also excel in a full-stack role where he can leverage AI to enhance user experience and application functionality.\\n*   **AI Research Engineer:** His project \"Harini: Intelligent AI Scheduling Agent\" and experience with LangChain and LangGraph show his interest and skills in developing intelligent AI agents, which is relevant to research-oriented roles.',\n",
       " 'chat_summary': ''}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     +-----------+           \n",
      "                     | __start__ |           \n",
      "                     +-----------+           \n",
      "                            *                \n",
      "                            *                \n",
      "                            *                \n",
      "                     +------------+          \n",
      "                     | rag_answer |          \n",
      "                     +------------+          \n",
      "                   ***             ...       \n",
      "                 **                   ..     \n",
      "               **                       ...  \n",
      "  +-----------------+                      ..\n",
      "  | summarizer_node |                       .\n",
      "  +-----------------+                       .\n",
      "            *                               .\n",
      "            *                               .\n",
      "            *                               .\n",
      "+----------------------+                   ..\n",
      "| hallucination_grader |                ...  \n",
      "+----------------------+              ..     \n",
      "                   ***             ...       \n",
      "                      **        ...          \n",
      "                        **    ..             \n",
      "                   +---------------+         \n",
      "                   | answer_grader |         \n",
      "                   +---------------+         \n",
      "                    ..            ...        \n",
      "                  ..                 ..      \n",
      "                ..                     ..    \n",
      "     +---------------+                   ..  \n",
      "     | fallback_node |                 ..    \n",
      "     +---------------+               ..      \n",
      "                    **            ...        \n",
      "                      **        ..           \n",
      "                        **    ..             \n",
      "                      +---------+            \n",
      "                      | __end__ |            \n",
      "                      +---------+            \n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(question='What job role is perfect for the candidiate', answer='', chat_summary='')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
